{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image caption generator\n",
    "\n",
    "Human brain can easily recognize an image and tell what the image is all about. But  a computer cannot do the same unless it is trained for it.Automatically describing the content of images using natural languages is a fundamental and challenging task.\n",
    "which will have various practical benefits in future ranging from Aiding the visually impaired to enabling the Automatic labelling of millions of images that gets uploaded to the internet everyday.\n",
    "So, to make our image caption generator model, we will be merging CNN-RNN architectures. \n",
    "Feature extraction from images is done using CNN. We have used the pre-trained model Exception. \n",
    "The information received from CNN is then used by LSTM for generating a description of the image.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Getting and performing data cleaning](#2)\n",
    "- [3 - Extracting the feature vector from all images](#3)\n",
    "- [4 - Loading dataset for Training the model](#4)\n",
    "- [5 -  Tokenizing the vocabulary](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages ##\n",
    "\n",
    "These are the fundametal package that are required for the project. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [keras](https://keras.io/) Python library for developing and evaluating deep learning models\n",
    "- [pickle](https://github.com/python/cpython/blob/3.9/Lib/pickle.py) is the module implements binary protocols for serializing and de-serializing a Python object structure.\n",
    "- [tqdm](https://tqdm.github.io/) is to show the progress of the loops.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test our model with our own picture at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5c4c202b04a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxception\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pickle import dump, load\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Getting and performing data cleaning\n",
    "The main text file which contains all image captions is Flickr8k.token in our Flickr_8k_text folder.\n",
    "The format of our file is image and caption separated by a new line (“\\n”).\n",
    "Each image has 5 captions and we can see that #(0 to 5)number is assigned for each caption.\n",
    "\n",
    "We will define 6 functions:\n",
    "\n",
    "**load_doc( filename )** – For loading the document file and reading the contents inside the file into a string.\n",
    "\n",
    "**all_img_captions( filename )** – This function will create a descriptions dictionary that maps images with a list of 5 captions.\n",
    "\n",
    "**cleaning_text( descriptions )** – This function takes all descriptions and performs data cleaning. we will be removing punctuations, converting all text to lowercase and removing words that contain numbers.\n",
    "\n",
    "**text_vocabulary( descriptions )** – This is a simple function that will separate all the unique words and create the vocabulary from all the descriptions.\n",
    "\n",
    "**save_descriptions( descriptions, filename )** – This function will create a list of all the descriptions that have been preprocessed and store them into a file.We will create a descriptions.txt file to store all the captions.\n",
    "\n",
    "**data_cleaning( dataset_text )** - get the image data and caption data, cleans it, dump it into files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.load_doc\n",
    "For loading the document file and reading the contents inside the file into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a text file into memory\n",
    "\n",
    "def load_doc(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        filename -- file containing all the captions.\n",
    "        \n",
    "    Returns:\n",
    "        text -- a string contains all the text inside the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Opening the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.all_img_captions\n",
    "This function will create a descriptions dictionary that maps images with a list of 5 captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all imgs with their captions\n",
    "\n",
    "def all_img_captions(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        filename -- file containing all the captions.\n",
    "        \n",
    "    Returns:\n",
    "        descriptions -- a dictionary contains image_name and corresponding description list.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = load_doc(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions ={}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [ caption ]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.cleaning_text\n",
    "This function takes all descriptions and performs data cleaning. we will be removing punctuations, converting all text to lowercase and removing words that contain numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning- lower casing, removing puntuations and words containing numbers\n",
    "\n",
    "def cleaning_text( captions ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        captions -- a dictionary contains image_name and corresponding description list.\n",
    "        \n",
    "    Returns:\n",
    "        captions -- cleaned (lower casing, removing puntuations and words containing numbers) version of given dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for img,caps in captions.items():\n",
    "        for i,img_caption in enumerate(caps):\n",
    "\n",
    "            img_caption.replace(\"-\",\" \")\n",
    "            desc = img_caption.split()\n",
    "\n",
    "            #converts to lowercase\n",
    "            desc = [word.lower() for word in desc]\n",
    "            #remove punctuation from each token\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            #remove hanging 's and a \n",
    "            desc = [word for word in desc if(len(word)>1)]\n",
    "            #remove tokens with numbers in them\n",
    "            desc = [word for word in desc if(word.isalpha())]\n",
    "            #convert back to string\n",
    "\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i]= img_caption\n",
    "    return captions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.text_vocabulary\n",
    "This is a simple function that will separate all the unique words and create the vocabulary from all the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary of all unique words\n",
    "\n",
    "def text_vocabulary(descriptions):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        descriptions -- cleaned version of given dictionary.\n",
    "        \n",
    "    Returns:\n",
    "        vocab -- set containing all unique words in the descriptions.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()\n",
    "\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.save_descriptions\n",
    "This function will create a list of all the descriptions that have been preprocessed and store them into a file.We will create a descriptions.txt file to store all the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All descriptions in one file \n",
    "def save_descriptions(descriptions, filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        descriptions -- cleaned version of given dictionary.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc )\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename,\"w\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.data_cleaning\n",
    "This function:\n",
    "\n",
    "get the caption file,cleans it and save it to descriptions.txt file.\n",
    "\n",
    "make the vocabulary from those decriptions and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the image data and caption data, cleans it, dump it into files.\n",
    "\n",
    "def data_cleaning( dataset_text ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dataset_text -- path to the dataset's text folder. \n",
    "        \n",
    "    Returns:\n",
    "        vocabulary -- set containing all unique words in the descriptions.\n",
    "    \"\"\"\n",
    "\n",
    "    #we prepare our text data\n",
    "    filename = dataset_text + \"/\" + \"check.txt\"\n",
    "    \n",
    "    #loading the file that contains all data\n",
    "    #mapping them into descriptions dictionary img to 5 captions\n",
    "    descriptions = all_img_captions(filename)\n",
    "    print(\"Length of descriptions =\" ,len(descriptions))\n",
    "\n",
    "    #cleaning the descriptions\n",
    "    clean_descriptions = cleaning_text(descriptions)\n",
    "\n",
    "    #building vocabulary \n",
    "    vocabulary = text_vocabulary(clean_descriptions)\n",
    "    print(\"Length of vocabulary = \", len(vocabulary))\n",
    "\n",
    "    #saving each description to file \n",
    "    save_descriptions(clean_descriptions, \"descriptions.txt\")\n",
    "    \n",
    "    return vocabulary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Extracting the feature vector from all images \n",
    "we use the pre-trained model that have been already trained on large datasets and extract the features from these models and use them for our tasks. We are using the Xception model which has been trained on imagenet dataset that had 1000 different classes to classify. We can directly import this model from the keras.applications . Make sure you are connected to the internet as the weights get automatically downloaded. Since the Xception model was originally built for imagenet, we will do little changes for integrating with our model. One thing to notice is that the Xception model takes 299*299*3 image size as input. We will remove the last classification layer and get the 2048 feature vector.\n",
    "\n",
    "The function\n",
    "**extract_features( image_directory )** will extract features for all images and we will map image names with their respective feature array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.extract_features\n",
    "The function will extract features for all images and we will map image names with their respective feature array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features for all images\n",
    "\n",
    "def extract_features( image_directory ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image_directory -- path to the dataset's image folder. \n",
    "        \n",
    "    Returns:\n",
    "        features -- map containing image name and corresponding feature array.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Xception( include_top=False, pooling='avg' )\n",
    "    features = {}\n",
    "    for img in tqdm(os.listdir(image_directory)):\n",
    "        filename = image_directory + \"/\" + img\n",
    "        image = Image.open(filename)\n",
    "        image = image.resize((299,299))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        #image = preprocess_input(image)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "\n",
    "        feature = model.predict(image)\n",
    "        features[img] = feature\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Loading dataset for Training the model\n",
    "In our Flickr_8k_test folder, we have Flickr_8k.trainImages.txt file that contains a list of 6000 image names that we will use for training.\n",
    "\n",
    "For loading the training dataset, we need more functions:\n",
    "\n",
    "**load_photos( filename )** – This will load the text file in a string and will return the list of image names.\n",
    "\n",
    "**load_clean_descriptions( filename, photos )** – This function will create a dictionary that contains captions for each photo from the list of photos.\n",
    "    \n",
    "**load_features( photos )** – This function will give us the dictionary for image names and their feature vector which we have previously extracted from the Xception model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.load_photos\n",
    "This will load the text file in a string and will return the list of image names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data \n",
    "\n",
    "def load_photos( filename ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        filename -- filename of the text file(which contains image name and corresponding description). \n",
    "        \n",
    "    Returns:\n",
    "        photos -- list of the images name.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"\\n\")[:-1]\n",
    "    return photos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.load_clean_descriptions\n",
    "This function will create a dictionary that contains captions for each photo from the list of photos. We also append the <start> and <end> identifier for each caption. We need this so that our LSTM model can identify the starting and ending of the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading clean_descriptions\n",
    "\n",
    "def load_clean_descriptions( filename, photos ): \n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        filename -- filename of the text file(which contains image name and corresponding description). \n",
    "        photos   -- list of the images name.\n",
    "        \n",
    "    Returns:\n",
    "        descriptions -- dictionary that contains captions for each photo from the list of photos.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"\\n\"):\n",
    "\n",
    "        words = line.split()\n",
    "        if len(words)<1 :\n",
    "            continue\n",
    "\n",
    "        image, image_caption = words[0], words[1:]\n",
    "\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "\n",
    "    return descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.load_features\n",
    "This function will give us the dictionary for image names and their feature vector which we have previously extracted from the Xception model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all features\n",
    "\n",
    "def load_features( photos ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        photos   -- list of the images name.\n",
    "        \n",
    "    Returns:\n",
    "        features -- dictionary for image names and their feature vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_features = load(open(\"features.p\",\"rb\"))\n",
    "    #selecting only needed features\n",
    "    features = {k:all_features[k] for k in photos}\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Tokenizing the vocabulary \n",
    "Computers don’t understand English words, for computers, we will have to represent them with numbers. So, we will map each word of the vocabulary with a unique index value. Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a “tokenizer.p” pickle file.\n",
    "\n",
    "Functions used are:\n",
    "\n",
    "**dict_to_list( descriptions )** - converting dictionary to clean list of descriptions.\n",
    "\n",
    "**create_tokenizer( descriptions )** - this will create the tokenizer object and fits on text and return the object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.dict_to_list\n",
    "converting dictionary to clean list of descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting dictionary to clean list of descriptions\n",
    "\n",
    "def dict_to_list( descriptions ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        descriptions -- dictionary that contains captions for each photo from the list of photos.\n",
    "        \n",
    "    Returns:\n",
    "        all_desc -- list of descriptions.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.create_tokenizer\n",
    "this will create the tokenizer object and fits on text and return the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tokenizer class \n",
    "#this will vectorise text corpus\n",
    "#each integer will represent token in dictionary\n",
    "\n",
    "def create_tokenizer( descriptions ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        descriptions -- dictionary that contains captions for each photo from the list of photos.\n",
    "        \n",
    "    Returns:\n",
    "        tokenizer -- tokenizer object.\n",
    "    \"\"\"\n",
    "    \n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set these path according to project folder in you system\n",
    "dataset_text = \"G:\\image_caption\\Flickr8k_text\"\n",
    "dataset_images = \"G:\\image_caption\\Flickr8k_Dataset\"\n",
    "\n",
    "#data cleaning\n",
    "vocabulary = data_cleaning(dataset_text)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "#2048 feature vector\n",
    "features = extract_features(dataset_images)\n",
    "dump(features, open(\"features.p\",\"wb\"))\n",
    "\n",
    "features = load(open(\"features.p\",\"rb\"))\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "\n",
    "#train = loading_data(filename)\n",
    "train_imgs = load_photos(filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)\n",
    "print(\"step 4 done \")\n",
    "\n",
    "##########################################################################################\n",
    "# give each word an index, and store that into tokenizer.p pickle file\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size\n",
    "print(\"step 5 done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
